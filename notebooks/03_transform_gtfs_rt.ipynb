{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e38af9-88e0-48fe-ba5d-cf5fb70f1cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 03 - Transform GTFS Real-Time Data (Silver Layer)\n",
    "\n",
    "Cleans and enriches real-time GTFS vehicle position data to prepare it for analysis.\n",
    "\n",
    "\n",
    "### Purpose\n",
    "Convert raw real-time transit signals into structured, deduplicated records with timestamps and locations for consistent downstream use.\n",
    "\n",
    "\n",
    "### Steps\n",
    "- Filters out invalid or null coordinates\n",
    "- Converts `timestamp` to `event_ts` and derives `event_date`\n",
    "- Adds `location` and processing metadata\n",
    "- Removes duplicate vehicle-timestamp pairs\n",
    "- Writes transformed data to: `dbfs:/silver/gtfs_rt/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6e99b5-ae64-4236-8f30-646631de2947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"dbfs:/silver/gtfs_rt/\", recurse=True)   One-time utility: This cell is only needed the very first time you switch from overwrite to append logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee66a5f-492f-4611-a725-e8b18b6176b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set today's ingestion date (used for filtering)\n",
    "INGESTION_DATE = dt.date.today().isoformat()\n",
    "\n",
    "# Define paths for Bronze and Silver Delta layers\n",
    "BRONZE_RT_PATH = \"dbfs:/bronze/gtfs_rt/\"\n",
    "SILVER_RT_PATH = \"dbfs:/silver/gtfs_rt/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f6e8a9-3057-4df0-aba8-908364c6c357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Bronze layer and filter today’s data with valid GPS and timestamp values\n",
    "df_bronze = (\n",
    "    spark.read.format(\"delta\").load(BRONZE_RT_PATH)\n",
    "    .filter(F.col(\"ingestion_date\") == INGESTION_DATE)\n",
    "    .filter((F.col(\"timestamp\") > 0))\n",
    "    .filter(F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())\n",
    "    .filter(~((F.col(\"latitude\") == 0.0) & (F.col(\"longitude\") == 0.0)))  \n",
    ")\n",
    "\n",
    "df_bronze.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac315e6-9579-4f45-bc6b-3691e91c0da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform Bronze data:\n",
    "# - Convert raw timestamp to datetime\n",
    "# - Create a separate event date column\n",
    "# - Combine lat/lon into a struct\n",
    "# - Add a processing timestamp\n",
    "# - Remove duplicate events by vehicle and timestamp\n",
    "df_silver = (\n",
    "    df_bronze\n",
    "    .withColumn(\"event_ts\", F.to_timestamp(F.col(\"timestamp\")))\n",
    "    .withColumn(\"event_date\", F.to_date(F.col(\"event_ts\")))\n",
    "    .withColumn(\"location\", F.struct(F.col(\"latitude\"), F.col(\"longitude\")))\n",
    "    .withColumn(\"processed_at\", F.current_timestamp())\n",
    "    .dropDuplicates([\"vehicle_id\", \"timestamp\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c8b82c-6ead-4b14-a468-961c139b725a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplicate based on vehicle_id and timestamp before writing to Silver\n",
    "try:\n",
    "    existing_silver = spark.read.format(\"delta\").load(SILVER_RT_PATH).select(\"vehicle_id\", \"timestamp\")\n",
    "    \n",
    "    df_silver = df_silver.alias(\"new\").join(\n",
    "        existing_silver.alias(\"existing\"),\n",
    "        on=[\"vehicle_id\", \"timestamp\"],\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"✓ No existing Silver data found or table is empty. Proceeding without anti-join. Error: {e}\")\n",
    "\n",
    "# Write the transformed Silver-layer data back to Delta format\n",
    "# Partitioned by ingestion date to enable efficient querying\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .save(SILVER_RT_PATH)\n",
    "\n",
    "print(\"✓ GTFS-RT Silver transform complete (duplicates avoided)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b65d6e-7c4c-4534-a273-81c08c83d873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check: show some of today’s newly written Silver-layer data\n",
    "spark.read.format(\"delta\").load(SILVER_RT_PATH) \\\n",
    "    .filter(F.col(\"ingestion_date\") == INGESTION_DATE) \\\n",
    "    .show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4fa9ed-9061-4c07-8335-80d525cd0275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check distinct dates in the Silver data to confirm successful partitioning\n",
    "spark.read.format(\"delta\").load(\"dbfs:/silver/gtfs_rt\").select(\"event_date\").distinct().orderBy(\"event_date\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991a8674-fda5-432c-97ad-32feda16cc5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Investigate if any default (Unix epoch) timestamps slipped through\n",
    "spark.read.format(\"delta\").load(\"dbfs:/silver/gtfs_rt\") \\\n",
    "    .filter(\"event_date = '1970-01-01'\") \\\n",
    "    .select(\"vehicle_id\", \"timestamp\", \"event_ts\", \"event_date\") \\\n",
    "    .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420fa4e8-2216-43d8-9aee-7dc0b3626b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if any records still have timestamp = 0 (which shouldn't happen after filtering)\n",
    "spark.read.format(\"delta\").load(\"dbfs:/silver/gtfs_rt\") \\\n",
    "    .filter(\"timestamp = 0\") \\\n",
    "    .count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce23e4d6-60bd-4b28-896c-da0e35653da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_transform_gtfs_rt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
