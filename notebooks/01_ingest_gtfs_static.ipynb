{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0c7bb2f-d269-428c-a4c8-bdd585fd98f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 01 - Ingest GTFS Static Data (Bronze Layer with SCD2)\n",
    "\n",
    "This notebook downloads and ingests the **static GTFS feed** from King County Metro into the **Bronze Delta Lake layer**, using a **Slowly Changing Dimension Type 2 (SCD2)** approach.\n",
    "\n",
    "### Purpose\n",
    "To extract and version core static transit reference data (routes, stops, trips) in a way that preserves historical changes over time. This data will be enriched and analyzed in later stages.\n",
    "\n",
    "### Workflow Summary\n",
    "- Downloads the latest GTFS static `.zip` file from [King County Metro GTFS](https://metro.kingcounty.gov/gtfs/)\n",
    "- Extracts and parses key `.txt` files (`routes.txt`, `stops.txt`, `trips.txt`)\n",
    "- Converts them into Spark DataFrames\n",
    "- Compares the new data with the latest existing Delta tables\n",
    "- Updates Delta tables using SCD2:\n",
    "  - Inserts new or changed records\n",
    "  - Marks outdated records with `end_time` and `is_current = false`\n",
    "- Stores each ingested table as a Delta Lake dataset in the Bronze layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23de1756-a9c0-45e1-9d55-b0259f48380d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Download and Ingest GTFS Static Files\n",
    "from pyspark.sql import functions as F\n",
    "import requests, zipfile, io, datetime as dt, os, shutil, tempfile   \n",
    "from pyspark.sql.functions import lit, current_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfebc7b-f976-4ae2-9a3a-84509bc93f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Just one time run in order to implement SCD2 for the historical data\n",
    "'''TODAY       = \"2025-05-21\"          # Static ingest date\n",
    "BRONZE_BASE = \"dbfs:/bronze\"\n",
    "BRONZE_PATH = f\"{BRONZE_BASE}/gtfs_static\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd9b9f5-3498-4c02-8703-fc0438cdd906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Just one time run in order to implement SCD2 for the historical data\n",
    "'''target_tables = {\n",
    "    \"routes.txt\": \"route_id\",\n",
    "    \"stops.txt\": \"stop_id\",\n",
    "    \"trips.txt\": \"trip_id\"\n",
    "}\n",
    "\n",
    "for name, key_col in target_tables.items():\n",
    "    table_name = name.replace(\".txt\", \"\")\n",
    "    table_path = f\"{BRONZE_PATH}/{table_name}\"\n",
    "    old_path = os.path.join(BRONZE_PATH, TODAY, table_name)\n",
    "   \n",
    "    df = spark.read.format(\"delta\").load(old_path)\n",
    "    df_new = df.withColumn(\"start_time\", lit(TODAY)) \\\n",
    "               .withColumn(\"end_time\", lit(None).cast(\"timestamp\")) \\\n",
    "               .withColumn(\"is_current\", lit(True)) \\\n",
    "               .withColumn(\"processed_at\", current_timestamp())\n",
    "    df_new.display()\n",
    "    df_new.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "    print(f\"‚úÖ Updated {table_name}\")'''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d792f4ad-2680-4c3b-915b-158d2b60f1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "import requests, zipfile, io, datetime as dt, os, shutil, tempfile  # dt alias\n",
    "\n",
    "# ---------- config ----------\n",
    "GTFS_URL    = \"https://metro.kingcounty.gov/gtfs/google_transit.zip\"\n",
    "BRONZE_BASE = \"dbfs:/bronze\"\n",
    "BRONZE_PATH = f\"{BRONZE_BASE}/gtfs_static/\"\n",
    "\n",
    "# Shared temp folder in DBFS where executors can read the files\n",
    "DBFS_TMP_DIR = \"dbfs:/tmp/gtfs_static\"          # lives in DBFS (NOT driver /tmp)\n",
    "dbutils.fs.mkdirs(DBFS_TMP_DIR)            # idempotent\n",
    "\n",
    "# Create driver‚Äëlocal temporary directory for the ZIP extraction\n",
    "tmp_dir = tempfile.mkdtemp(prefix=\"gtfs_\")\n",
    "print(f\"Driver temp dir: {tmp_dir}\")\n",
    "# Define tables and corresponding key columns\n",
    "target_tables = {\n",
    "    \"routes.txt\": \"route_id\",\n",
    "    \"stops.txt\": \"stop_id\",\n",
    "    \"trips.txt\": \"trip_id\"\n",
    "}\n",
    "try:\n",
    "    print(\"Downloading GTFS zip ‚Ä¶\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(requests.get(GTFS_URL, timeout=30).content))\n",
    "    for name, key_col in target_tables.items():\n",
    "        table_name = name.replace(\".txt\", \"\")\n",
    "        table_path = f\"{BRONZE_BASE}/gtfs_static/{table_name}\"\n",
    "\n",
    "        # Skip tables not in zip (if any)\n",
    "        if name not in z.namelist():\n",
    "            print(f\"‚ö†Ô∏è Skipping {name} ‚Äî not found in zip.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üì• Processing {name} with key column '{key_col}'\")\n",
    "\n",
    "        # Extract and read\n",
    "        local_file = os.path.join(tmp_dir, name)\n",
    "        with z.open(name) as src, open(local_file, \"wb\") as dst:\n",
    "            dst.write(src.read())\n",
    "        print(\"z open successful!\", name)\n",
    "        dbfs_file = f\"{DBFS_TMP_DIR}/{name}\"\n",
    "        with open(local_file, \"r\") as f:\n",
    "            dbutils.fs.put(dbfs_file, f.read(), overwrite=True)\n",
    "        df = (\n",
    "            spark.read.option(\"header\", True)\n",
    "            .csv(dbfs_file)\n",
    "            )\n",
    "        # Add SCD2 columns\n",
    "        df_new = df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                .withColumn(\"end_time\", lit(None).cast(\"timestamp\")) \\\n",
    "                .withColumn(\"is_current\", lit(True)) \\\n",
    "                .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "        try:\n",
    "            df_existing = spark.read.format(\"delta\").load(table_path)\n",
    "            existing_cols = set(df_existing.columns)\n",
    "            new_cols      = set(df_new.columns)\n",
    "\n",
    "            cols_missing_in_new = existing_cols - new_cols     \n",
    "            cols_missing_in_table = new_cols - existing_cols\n",
    "            if \"df_existing\" in locals():          # only runs when the table already exists\n",
    "                all_cols = set(df_existing.columns).union(cols_missing_in_table)   # every column currently in the SCD2 table\n",
    "                #print (all_cols)\n",
    "                #print (cols_missing_in_new)\n",
    "                #print (cols_missing_in_table)\n",
    "                # Build a column list: use real column when present, otherwise a NULL placeholder\n",
    "                df_new = df_new.select([\n",
    "                    F.col(c) if c in df_new.columns\n",
    "                    else F.lit(None).cast(\"string\").alias(c)   # <-- adjust cast if needed\n",
    "                    for c in all_cols\n",
    "                ])\n",
    "\n",
    "                # Build a column list: use real column when present, otherwise a NULL placeholder\n",
    "                df_existing = df_existing.select([\n",
    "                    F.col(c) if c in df_existing.columns\n",
    "                    else F.lit(None).cast(\"string\").alias(c)   # <-- adjust cast if needed\n",
    "                    for c in all_cols\n",
    "                ])\n",
    "\n",
    "\n",
    "            compare_cols = [c for c in df.columns if c != key_col]\n",
    "            df_existing_current = df_existing.filter(\"is_current\")\n",
    "\n",
    "            diff_condition = \" OR \".join([f\"new.{c} != existing.{c}\" for c in compare_cols])\n",
    "\n",
    "            df_changes = (\n",
    "                df_new.alias(\"new\")\n",
    "                .join(df_existing_current.alias(\"existing\"), on=key_col, how=\"left\")\n",
    "                .filter(diff_condition)\n",
    "                .select(\"new.*\")\n",
    "            )\n",
    "\n",
    "            if df_changes.count() > 0:\n",
    "                df_to_expire = (\n",
    "                    df_existing_current.alias(\"existing\")\n",
    "                    .join(df_changes.alias(\"changes\"), on=key_col, how=\"inner\")\n",
    "                    .select(\"existing.*\")                           # Select all columns from the existing (current) row\n",
    "                    .withColumn(\"end_time\", current_timestamp())    # Mark when this record was expired\n",
    "                    .withColumn(\"is_current\", lit(False))           # Set is_current = False\n",
    "                    .withColumn(\"processed_at\", current_timestamp()) # Optionally track processing time\n",
    "                    ) \n",
    "\n",
    "\n",
    "                df_final_current = df_existing_current.join(\n",
    "                    df_to_expire.select(key_col), on=key_col, how=\"left_anti\"\n",
    "                )\n",
    "                df_historical = df_existing.filter(\"is_current = false\")\n",
    "                #print(\"df final current columns:\", df_final_current.columns)\n",
    "                #print(f\"df historical columns:\", df_historical.columns)\n",
    "                #print(f\"df to expire columns:\", df_to_expire.columns)\n",
    "                #print(f\"df changes columns:\", df_changes.columns)\n",
    "                df_final = (df_final_current\n",
    "                            .unionByName(df_to_expire)\n",
    "                            .unionByName(df_changes)\n",
    "                            .unionByName(df_historical)\n",
    "                )\n",
    "                df_final.display()\n",
    "                df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(table_path)\n",
    "                print(f\"‚úÖ Updated {table_name} (SCD2)\")\n",
    "\n",
    "            else:\n",
    "                print(f\"‚úì No changes found in {table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print(f\"‚úì Creating new SCD2 table for {table_name} with exception!\")\n",
    "            #df_new.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading GTFS zip: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e97575-ecf0-48cd-99c3-3aea51195466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(tmp_dir, ignore_errors=True) # Clean up temp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947832b4-c4ac-45eb-bc5a-b150ebfaa96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_gtfs_static",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
